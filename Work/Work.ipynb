{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy+Zhang Liangjun\n",
    "#verson 0.1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "#文件路径\n",
    "#filepath = '/home/gcc/dataset/test-xyz_depth.mat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input:mat file\n",
    "#output: tensor\n",
    "def mat_read(filepath):\n",
    "    dataFile = filepath\n",
    "    data = scio.loadmat(dataFile)\n",
    "    #读取mat里的depth数据\n",
    "    depth = data['depth']\n",
    "    #归一化\n",
    "    depth_scale=depth[depth>0]\n",
    "    avrg=np.mean(depth_scale)\n",
    "    var=np.std(depth_scale)\n",
    "    index=depth==0\n",
    "    depth[index]=avrg\n",
    "    depth_scale=(depth-avrg)/var\n",
    "    ##TODO\n",
    "    #where_are_nan = np.isnan(depth)\n",
    "    #depth[where_are_nan] = 0\n",
    "    #变成三通道\n",
    "    depth3=[]\n",
    "    for i in range(3):\n",
    "        depth3.append(depth_scale)\n",
    "    depth3=np.array(depth3)\n",
    "    #create tensor from numpy.ndarray\n",
    "    depth=torch.from_numpy(depth3)\n",
    "    depth=depth.type(torch.FloatTensor)\n",
    "    return depth\n",
    "\n",
    "#把数据文件分成训练集和测试集\n",
    "def generate_datasets(data_dir, dst_dir, train_ratio=0.7):\n",
    "    train_dir = os.path.join(dst_dir,'train')\n",
    "    val_dir = os.path.join(dst_dir,'val')\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.mkdir(dst_dir)\n",
    "        os.mkdir(train_dir)\n",
    "        os.mkdir(val_dir)\n",
    "    for root, dirnames, _ in os.walk(data_dir):\n",
    "        for dirname in dirnames:\n",
    "            #因为需要的数据在更下一层的depth文件里，所以加了几句，后边带俩#的为改过的部分，\n",
    "            #如果不需要做depth改过来就行了\n",
    "            #depth_dirname=os.path.join(dirname,'depth')##\n",
    "            subdirname_train = os.path.join(train_dir, dirname)\n",
    "            subdirname_val = os.path.join(val_dir, dirname)\n",
    "            if not os.path.exists(subdirname_train):\n",
    "                os.mkdir(subdirname_train)\n",
    "            if not os.path.exists(subdirname_val):\n",
    "                os.mkdir(subdirname_val)                                        \n",
    "            #dname = os.path.join(root, depth_dirname)##depth_dirname原本是dirname\n",
    "            dname = os.path.join(root, dirname)\n",
    "            names = glob.glob(dname+r'/*.mat')  \n",
    "            random.shuffle(names)\n",
    "            names_len = len(names)\n",
    "            train_names = names[:int(names_len*train_ratio)]\n",
    "            val_names = names[int(names_len*train_ratio)+1:]\n",
    "            for f in train_names:\n",
    "                fname = os.path.split(f)[-1]\n",
    "                train_dname = os.path.join(subdirname_train, fname)\n",
    "                shutil.copyfile(f, train_dname)\n",
    "            for f in val_names:\n",
    "                fname = os.path.split(f)[-1]\n",
    "                val_dname = os.path.join(subdirname_val, fname)\n",
    "                shutil.copyfile(f, val_dname)\n",
    "            print ('copy {} done'.format(dname))\n",
    "        \n",
    "data_dir = '/home/sjtu/gcj/data/depth_data/depthmat2'\n",
    "dst_dir = '/home/sjtu/gcj/data/depth_data/depthmat_new'\n",
    "#generate_datasets(data_dir, dst_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXTENSIONS = ['.mat']\n",
    "def is_mat_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in EXTENSIONS)\n",
    "\n",
    "#类名\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir,d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]:i for i in range(len(classes))}\n",
    "    return classes,class_to_idx\n",
    "\n",
    "#input: dir+train(or val)+class\n",
    "#output: 数据文件的集合\n",
    "def make_dataset(dir,phase,class_to_idx):\n",
    "    datas = []\n",
    "    dir = os.path.join(dir,phase)\n",
    "    for target in os.listdir(dir):\n",
    "        d = os.path.join(dir,target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        \n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in fnames:\n",
    "                if is_mat_file(fname):\n",
    "                    path = os.path.join(root,fname)\n",
    "                    item = (path,class_to_idx[target])\n",
    "                    datas.append(item)\n",
    "    return datas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ViewpointDataset(data.Dataset):\n",
    "    def __init__(self, root, transform = None, phase = None):\n",
    "        dir = os.path.join(root, phase)\n",
    "        classes, class_to_idx = find_classes(dir)\n",
    "        datas = make_dataset(root,phase, class_to_idx)\n",
    "        if len(datas) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported extensions are: \" + \",\".join(EXTENSIONS)))\n",
    "        self.root = root\n",
    "        self.datas = datas\n",
    "        self.phase = phase\n",
    "        self.classes = classes\n",
    "        #todo\n",
    "        self.width = 480\n",
    "        self.height = 640\n",
    "        self.suffix = '.mat'\n",
    "        self.transform = transform\n",
    "        \n",
    "    #深度矩阵转成tensor  \n",
    "    def __getitem__(self, idx):\n",
    "        mat_path, label = self.datas[idx]\n",
    "        #preprocess\n",
    "        depth_tensor = mat_read(mat_path)            \n",
    "        \n",
    "        #numpy转成tensor  (不用了，matread里边转过tensor了)\n",
    "        #depth_tensor = torch.from_numpy(depth)\n",
    "        return depth_tensor, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 24864, 'val': 10619}\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "#建立数据集\n",
    "dsets = {x: ViewpointDataset(dst_dir,phase=x) for x in ['train', 'val']}\n",
    "dset_loaders = {x:torch.utils.data.DataLoader(dsets[x],batch_size=24,shuffle=True, num_workers=8) for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_classes = dsets['val'].classes\n",
    "print(dset_sizes)\n",
    "print(len(dset_classes))\n",
    "use_gpu = torch.cuda.is_available()\n",
    "#use_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#设置梯度更新方式\n",
    "def optim_scheduler_ft(model, epoch, init_lr=0.001, lr_decay_epoch=7):\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet (\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (relu): ReLU (inplace)\n",
      "  (maxpool): MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
      "  (layer1): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (2): BasicBlock (\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (2): BasicBlock (\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (3): BasicBlock (\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (2): BasicBlock (\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (3): BasicBlock (\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (4): BasicBlock (\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (5): BasicBlock (\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (2): BasicBlock (\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d (\n",
      "  )\n",
      "  (fc): Linear (512 -> 1000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#建立模型\n",
    "model = models.resnet34(pretrained=False)\n",
    "#model还可以是各种卷积网络结构，到时候调整\n",
    "#model = torch.load(\"model_epoch25.pkl\")\n",
    "print (model)\n",
    "num_ftrs = model.fc.in_features\n",
    "classes = len(dset_classes)\n",
    "model.fc = nn.Linear(2048, classes)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#删除最后一层\n",
    "#new_classifier = nn.Sequential(*list(model.children())[:-1])\n",
    "#print (new_classifier)\n",
    "\n",
    "#测试网络\n",
    "#a=mat_read('/home/gcc/viewpoint/off_1_90_0/depth/1_depth.mat')\n",
    "#c=torch.FloatTensor(1,3,480,640)#需要写成3dtensor 1代表batch\n",
    "#c[0]=a\n",
    "#b=Variable(c.cuda())\n",
    "#print(type(b))\n",
    "#outputs = new_classifier(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##定义模型如何训练\n",
    "def train_model(model, criterion, optim_scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                optimizer = optim_scheduler(model, epoch)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dset_loaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                #print(type(inputs))\n",
    "                # wrap them in Variable\n",
    "                if use_gpu and phase=='train':\n",
    "                    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                elif use_gpu and phase=='val':\n",
    "                    inputs, labels = Variable(inputs.cuda(),volatile=True), Variable(labels.cuda(),volatile=True)\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                #print(type(inputs))\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds== labels.data)\n",
    "                #print(running_loss)\n",
    "\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = float(running_corrects) / float(dset_sizes[phase])\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "LR is set to 0.001\n",
      "train Loss: 0.0301 Acc: 0.7734\n",
      "val Loss: 0.0061 Acc: 0.9516\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9571\n",
      "val Loss: 0.0018 Acc: 0.9847\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.9809\n",
      "val Loss: 0.0015 Acc: 0.9885\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9879\n",
      "val Loss: 0.0010 Acc: 0.9914\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9937\n",
      "val Loss: 0.0002 Acc: 0.9982\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9939\n",
      "val Loss: 0.0015 Acc: 0.9866\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9951\n",
      "val Loss: 0.0008 Acc: 0.9937\n",
      "Epoch 7/24\n",
      "----------\n",
      "LR is set to 0.0001\n",
      "train Loss: 0.0001 Acc: 0.9996\n",
      "val Loss: 0.0000 Acc: 0.9998\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 0.9999\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 0.9999\n",
      "val Loss: 0.0000 Acc: 0.9999\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 14/24\n",
      "----------\n",
      "LR is set to 1e-05\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 21/24\n",
      "----------\n",
      "LR is set to 1e-06\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "val Loss: 0.0000 Acc: 1.0000\n",
      "Training complete in 226m 26s\n",
      "Best val Acc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "model = train_model(model, criterion, optim_scheduler_ft, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##在测试集中测试\n",
    "def visualize_model(model, num_images=5):\n",
    "    for i, data in enumerate(dset_loaders['val']):\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        preds = preds.cpu().numpy()\n",
    "        print dset_classes[preds[0][0]]\n",
    "\n",
    "        if i == num_images - 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#单个图测试\n",
    "def predict_depth_mat(model,dep_path):\n",
    "    inputs = mat_read(dep_path)\n",
    "    if use_gpu:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "    outputs = model(inputs)\n",
    "    _,preds = torch.max(outputs.data, 1)\n",
    "    preds = preds.cpu().numpy()\n",
    "    print dset_classes[preds[0][0]]\n",
    "#等到知道真实深度图是什么类型的数据和怎么处理后再写\n",
    "def predict_depthmap(model,image):\n",
    "    pass\n",
    "#filename = 'mat文件地址'\n",
    "#predict_depth_mat(model,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#保存模型\n",
    "#torch.save(model, 'model_epoch50.pkl')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
