{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy+Zhang Liangjun\n",
    "#verson 1.0\n",
    "#class 2classes\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "import csv\n",
    "import math\n",
    "import pandas\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input:mat file\n",
    "#output: tensor\n",
    "def mat_read(filepath):\n",
    "    dataFile = filepath\n",
    "    data = scio.loadmat(dataFile)\n",
    "    #读取mat里的depth数据\n",
    "    depth = data['depth']\n",
    "    #归一化\n",
    "    depth=depth*300\n",
    "    depth=depth.astype(np.int)\n",
    "    depth=depth.astype(np.float)\n",
    "    #depth = transform.resize(depth,(240,320))\n",
    "    depth_scale=depth[depth>0]\n",
    "    avrg=np.mean(depth_scale)\n",
    "    var=np.std(depth_scale)\n",
    "    index=depth==0\n",
    "    depth[index]=avrg\n",
    "    depth_scale=(depth-avrg)/var\n",
    "    depth=np.array(depth_scale)\n",
    "\n",
    "    depth=np.expand_dims(depth,0)\n",
    "    return depth\n",
    "#把数据文件分成训练集和测试集\n",
    "\n",
    "dst_dir = '/home/sjtu/gcj/data/depth_data/depth_26class_target'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXTENSIONS = ['.mat','.csv']\n",
    "def is_mat_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in EXTENSIONS)\n",
    "\n",
    "#类名\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir,d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]:i for i in range(len(classes))}\n",
    "    return classes,class_to_idx\n",
    "\n",
    "#input: dir+train(or val)+class\n",
    "#output: 数据文件的集合\n",
    "def make_dataset(dir,phase,class_to_idx):\n",
    "    datas = []\n",
    "    labels = []\n",
    "    dir = os.path.join(dir,phase)\n",
    "    for target in os.listdir(dir):\n",
    "        d = os.path.join(dir,target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        \n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in fnames:\n",
    "                if is_mat_file(fname):\n",
    "                    path = os.path.join(root,fname)\n",
    "                    #depth=mat_read(path)\n",
    "                    item = (path,class_to_idx[target])\n",
    "                    datas.append(item)\n",
    "                    #datas.append(depth)\n",
    "                    #labels.append(class_to_idx[target])\n",
    "    return datas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ViewpointDataset(data.Dataset):\n",
    "    def __init__(self, root, transform = None, phase = None):\n",
    "        dir = os.path.join(root, phase)\n",
    "        classes, class_to_idx = find_classes(dir)\n",
    "        datas= make_dataset(root,phase, class_to_idx)\n",
    "        if len(datas) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported extensions are: \" + \",\".join(EXTENSIONS)))\n",
    "        self.root = root\n",
    "        self.phase = phase\n",
    "        self.classes = classes\n",
    "        #todo\n",
    "        self.width = 480\n",
    "        self.height = 640\n",
    "        self.suffix = '.mat'\n",
    "        self.transform = transform\n",
    "        self.datas=datas\n",
    "        \n",
    "    #深度矩阵转成tensor  \n",
    "    def __getitem__(self, idx):\n",
    "        mat_path, label = self.datas[idx]\n",
    "        if self.phase == 'target':\n",
    "            with open(mat_path) as f:\n",
    "                l=[]\n",
    "                lines=csv.reader(f)\n",
    "                for line in lines:\n",
    "                    l.append(line)\n",
    "            l.remove(l[0])\n",
    "            l=np.array(l)\n",
    "            depth=l[:,1:]\n",
    "            depth=depth.astype(np.float)\n",
    "            depth_scale=depth[depth>0]\n",
    "            avrg=np.mean(depth_scale)\n",
    "            var=np.std(depth_scale)\n",
    "            index=depth==0\n",
    "            depth[index]=avrg\n",
    "            depth_scale=(depth-avrg)/var\n",
    "            depth=np.expand_dims(depth_scale,0)\n",
    "        else:\n",
    "        #preprocess\n",
    "            depth= mat_read(mat_path)\n",
    "        #depth,label=self.datas[idx]\n",
    "        #depth=self.datas[idx]\n",
    "        #label=self.labels[idx]\n",
    "\n",
    "\n",
    "        #create tensor from numpy.ndarray\n",
    "        depth=torch.from_numpy(depth)\n",
    "        depth_tensor=depth.type(torch.FloatTensor)\n",
    "        if self.transform:\n",
    "            toPIL=transforms.ToPILImage()\n",
    "            toTensor=transforms.ToTensor()\n",
    "            depth_tensor = toTensor(self.transform(toPIL(depth_tensor)))\n",
    "        return depth_tensor, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    #设置梯度更新方式\n",
    "    #增大学习率是个好办法\n",
    "def optim_scheduler_ft(model, epoch, init_lr=0.0005, lr_decay_epoch=7):\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "    #lr = init_lr\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 76436, 'target': 520, 'val': 32733}\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "#建立数据集\n",
    "batch_size=64\n",
    "dsets = {x: ViewpointDataset(dst_dir,phase=x) for x in ['target','train', 'val']}\n",
    "dset_loaders = {x:torch.utils.data.DataLoader(dsets[x],batch_size=batch_size,shuffle=True, num_workers=8) for x in ['target','train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['target','train', 'val']}\n",
    "dset_classes = dsets['val'].classes\n",
    "print(dset_sizes)\n",
    "print(len(dset_classes))\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (conv2): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc4): Linear (3072 -> 512)\n",
      "  (fc1): Linear (512 -> 128)\n",
      "  (adap): Linear (128 -> 128)\n",
      "  (fc3): Linear (128 -> 26)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\na=mat_read('/home/sjtu/gcj/data/depth_data/depth_26class/train/off_1_90_0/1_depth.mat')\\na=torch.from_numpy(a)\\na=a.type(torch.FloatTensor)\\nc=torch.FloatTensor(1,1,480,640)#\\xe9\\x9c\\x80\\xe8\\xa6\\x81\\xe5\\x86\\x99\\xe6\\x88\\x903dtensor 1\\xe4\\xbb\\xa3\\xe8\\xa1\\xa8batch\\nc[0]=a\\nb=Variable(c.cuda())\\nprint(type(b))\\noutputs,out1 = model(b)\\nprint outputs,out1\\n#\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练所用网络模型：\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(11,11),stride=(4,4),padding=(2,2)) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.fc4   = nn.Linear(3072, 512)\n",
    "        self.fc1   = nn.Linear(512,128)\n",
    "        self.adap   = nn.Linear(128,128) \n",
    "        self.fc3   = nn.Linear(128, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 4) # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 3) # If the size is a square you can only specify a single number\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv5(x)), 3) \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        #'''\n",
    "        x = F.dropout(x,p=0.5)\n",
    "        x=F.relu(self.fc4(x))\n",
    "        x=  F.dropout(x,p=0.5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.adap(x))\n",
    "        out1=x\n",
    "        x = self.fc3(x)\n",
    "        #'''\n",
    "        return x,out1\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "model = Net().cuda()\n",
    "print(model)\n",
    "'''\n",
    "a=mat_read('/home/sjtu/gcj/data/depth_data/depth_26class/train/off_1_90_0/1_depth.mat')\n",
    "a=torch.from_numpy(a)\n",
    "a=a.type(torch.FloatTensor)\n",
    "c=torch.FloatTensor(1,1,480,640)#需要写成3dtensor 1代表batch\n",
    "c[0]=a\n",
    "b=Variable(c.cuda())\n",
    "print(type(b))\n",
    "outputs,out1 = model(b)\n",
    "print outputs,out1\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def real_data(filepath):\n",
    "def test(csv_path):\n",
    "    with open(csv_path) as f:\n",
    "        l=[]\n",
    "        lines=csv.reader(f)\n",
    "        for line in lines:\n",
    "            l.append(line)\n",
    "    l.remove(l[0])\n",
    "    l=np.array(l)\n",
    "    depth=l[:,1:]\n",
    "    depth=depth.astype(np.float)\n",
    "    \n",
    "    depth_scale=depth[depth>0]\n",
    "    avrg=np.mean(depth_scale)\n",
    "    var=np.std(depth_scale)\n",
    "    index=depth==0\n",
    "    depth[index]=avrg\n",
    "    depth_scale=(depth-avrg)/var\n",
    "    depth3=np.expand_dims(depth_scale,0)\n",
    "    depth3=np.expand_dims(depth3,0)\n",
    "    #create tensor from numpy.ndarray\n",
    "    depth=torch.from_numpy(depth3)\n",
    "    depth=depth.type(torch.FloatTensor)\n",
    "    return depth,depth_scale\n",
    "\n",
    "def test_model(modelname,dir_name):\n",
    "    for root, dirnames, _ in os.walk(dir_name):\n",
    "        if len(dirnames)!=0:\n",
    "            count=0\n",
    "            acc=0\n",
    "            length = 0\n",
    "            for dirname in dirnames:\n",
    "                this_acc = 0\n",
    "                dname = os.path.join(root, dirname)\n",
    "                names=glob.glob(dname+r'/*.csv')\n",
    "                this_length  = len(names)\n",
    "                #model=torch.load(\"./model_2class.pth\")\n",
    "                model=torch.load(modelname)\n",
    "                model.cuda()\n",
    "                length=len(names)+length\n",
    "                for f in names:\n",
    "                    fname = os.path.split(f)[-1]\n",
    "                    csv_path = os.path.join(dname, fname)\n",
    "                    inputs,depth_scale=test(csv_path)\n",
    "                    label=dirname\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    outputs,_ = model(inputs)\n",
    "                    #print outputs.data\n",
    "                    _,preds = torch.max(outputs.data, 1)\n",
    "                    preds = preds.cpu().numpy()\n",
    "                    #print preds\n",
    "                    #preds=[[12]]\n",
    "                    #plt.imshow(preds)\n",
    "                    #print dset_classes[preds[0][0]]\n",
    "                    #plt.figure(count)\n",
    "                    count=count+1\n",
    "                    title_name=dset_classes[preds[0][0]].split('_')[2]+'_'+dset_classes[preds[0][0]].split('_')[3]\n",
    "                    if label==title_name:\n",
    "                        acc=acc+1\n",
    "                        this_acc = this_acc+1\n",
    "                    this_accuracy=float(this_acc)/float(this_length)\n",
    "                    #print('{}`s Acc: {:.4f}'.format(dirname,this_accuracy))\n",
    "            accuracy=float(acc)/float(length)\n",
    "            print('{}`s Acc: {:.4f}'.format(modelname,accuracy))\n",
    "            #plt.title(str(count)+' '+dset_classes[preds[0][0]])\n",
    "            #plt.title(title_name)\n",
    "            #plt.imshow(depth_scale)\n",
    "    return accuracy,modelname\n",
    "\n",
    "def visual_best_model(modelname,dir_name):\n",
    "    for root, dirnames, _ in os.walk(dir_name):\n",
    "        if len(dirnames)!=0:\n",
    "            count=0\n",
    "            acc=0\n",
    "            length = 0\n",
    "            for dirname in dirnames:\n",
    "                this_acc = 0\n",
    "                dname = os.path.join(root, dirname)\n",
    "                names=glob.glob(dname+r'/*.csv')\n",
    "                this_length  = len(names)\n",
    "                #model=torch.load(\"./model_2class.pth\")\n",
    "                model=torch.load(modelname)\n",
    "                model.cuda()\n",
    "                length=len(names)+length\n",
    "                for f in names:\n",
    "                    fname = os.path.split(f)[-1]\n",
    "                    csv_path = os.path.join(dname, fname)\n",
    "                    inputs,depth_scale=test(csv_path)\n",
    "                    label=dirname\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    outputs,_ = model(inputs)\n",
    "                    #print outputs.data\n",
    "                    _,preds = torch.max(outputs.data, 1)\n",
    "                    preds = preds.cpu().numpy()\n",
    "                    #print preds\n",
    "                    #preds=[[12]]\n",
    "                    #plt.imshow(preds)\n",
    "                    #print dset_classes[preds[0][0]]\n",
    "                    plt.figure(count)\n",
    "                    count=count+1\n",
    "                    title_name=dset_classes[preds[0][0]].split('_')[2]+'_'+dset_classes[preds[0][0]].split('_')[3]\n",
    "                    if label==title_name:\n",
    "                        acc=acc+1\n",
    "                        this_acc = this_acc+1\n",
    "                    plt.title(title_name)\n",
    "                    plt.imshow(depth_scale)\n",
    "                this_accuracy=float(this_acc)/float(this_length)\n",
    "                print('{}`s Acc: {:.4f}'.format(dirname,this_accuracy))\n",
    "            accuracy=float(acc)/float(length)\n",
    "            print('{}`s Acc: {:.4f}'.format(modelname,accuracy))\n",
    "            print('total:{}, wrong:{}'.format(length,length-acc))\n",
    "            #plt.title(str(count)+' '+dset_classes[preds[0][0]])\n",
    "    return accuracy,modelname\n",
    "def get_L(n_src,n_tar):\n",
    "    L_ss=(1./(n_src*n_src))*torch.ones(n_src,n_src)\n",
    "    L_st=(-1./(n_src*n_tar))*torch.ones(n_src,n_tar)\n",
    "    L_ts=(-1./(n_tar*n_src))*torch.ones(n_tar,n_src)\n",
    "    L_tt=(1./(n_tar*n_tar))*torch.ones(n_tar,n_tar)\n",
    "    L=torch.zeros(n_src+n_tar,n_src+n_tar)\n",
    "    L[:n_src,:n_src]=L_ss\n",
    "    L[:n_src,n_src:]=L_st\n",
    "    L[n_src:,:n_src]=L_ts\n",
    "    L[n_src:,n_src:]=L_tt\n",
    "    return L\n",
    "def get_kernel(x1, kernelparam=1, kerneltype='rbf'):\n",
    "    row,col=x1.size()[0],x1.size()[1]\n",
    "    K=None\n",
    "    if kerneltype=='rbf':\n",
    "        P=torch.sum(x1*x1,dim=1)\n",
    "        P=P.resize(len(P),1)\n",
    "        K=torch.exp(-1*(P.transpose(0,1).repeat(row,1)+P.repeat(1,row)-2*torch.mm(x1,x1.transpose(0,1)))/(col*2*kernelparam))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _mix_rbf_kernel(X, Y, sigma_list):\n",
    "    assert(X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    ZZT = torch.mm(Z, Z.t())\n",
    "    diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "    Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "    exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "    K = 0.0\n",
    "    for sigma in sigma_list:\n",
    "        gamma = 1.0 / (2 * sigma**2)\n",
    "        K += torch.exp(-gamma * exponent)\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:], len(sigma_list)\n",
    "\n",
    "\n",
    "def mix_rbf_mmd2(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n",
    "def _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "            + Kt_YY_sum / (m * (m - 1))\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    return mmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##定义模型如何训练\n",
    "def train_model(model, criterion, optim_scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "        \n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    count=0\n",
    "    #target_data\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        target_data=[]\n",
    "        for data in dset_loaders['target']:\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            # wrap them in Variable\n",
    "            target_data.append(inputs)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase=='train':\n",
    "                optimizer = optim_scheduler(model, epoch)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_mmd=0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dset_loaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                # wrap them in Variable\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                # zero the parameter gradients\n",
    "                # forward\n",
    "                mmd_loss=0\n",
    "                outputs,src = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                tar_inputs=target_data[random.randint(0,7)]\n",
    "                tar_inputs=Variable(tar_inputs.cuda(),requires_grad=False)\n",
    "                tar_out,tar=model(tar_inputs)\n",
    "                #mmd-rbf\n",
    "                '''src=src.size()[0]\n",
    "                n_tar=tar.size()[0]\n",
    "                L=get_L(n_src,n_tar)\n",
    "                L=Variable(L.cuda())\n",
    "                row,col=src.size()[0]+tar.size()[0],src.size()[1]\n",
    "                X=Variable(torch.zeros(row,col).cuda())\n",
    "                X[:src.size()[0],:]=src\n",
    "                X[src.size()[0]:,:]=tar\n",
    "                K=get_kernel(X)\n",
    "                A=torch.mm(K,L)\n",
    "                mmd_loss=A.trace()\n",
    "                \n",
    "                '''\n",
    "                sigma_list=[1,2,4,8,16]\n",
    "                if src.size(0)==tar.size(0):\n",
    "                    mmd2_D = mix_rbf_mmd2(src, tar, sigma_list)\n",
    "                    mmd2_D = F.relu(mmd2_D)\n",
    "                    mmd_loss=mmd2_D\n",
    "                else:\n",
    "                    mmd_loss=Variable(torch.Tensor([0.0]).cuda())\n",
    "                \n",
    "                #print mmd_loss\n",
    "                #if epoch%2==0:\n",
    "                loss = criterion(outputs, labels)\n",
    "                #else:\n",
    "                    #loss = mmd_loss\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_mmd += mmd_loss.data[0]\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds== labels.data)\n",
    "                #print(running_loss)\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_mmd = running_mmd / dset_sizes[phase]\n",
    "            final_loss=epoch_loss\n",
    "            epoch_acc = float(running_corrects) / float(dset_sizes[phase])\n",
    "            print epoch_mmd\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            # deep copy the model\n",
    "            #if phase == 'val' and epoch_acc >= best_acc:\n",
    "            if phase == 'val':\n",
    "                best_acc = epoch_acc\n",
    "                lowest_loss=final_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "        model_name='./model/model_26class_'+str(count)+'.pth'\n",
    "        dname='/home/sjtu/gcj/data/crop_test'\n",
    "        torch.save(best_model,model_name)\n",
    "        acc,modeltemp=test_model(model_name,dname)\n",
    "        count=count+1\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    torch.save(best_model, './model/model_26class.pth')\n",
    "    print('done')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 round\n",
      "Epoch 0/24\n",
      "----------\n",
      "LR is set to 0.0005\n"
     ]
    }
   ],
   "source": [
    "dname='/home/sjtu/gcj/data/crop_test'\n",
    "model_dir = './model'\n",
    "train_num = 10\n",
    "count = 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs=25\n",
    "for i in range(train_num):\n",
    "#训练\n",
    "    print(\"{} round\".format(count+1))\n",
    "    model = Net().cuda()\n",
    "    model = train_model(model,criterion, optim_scheduler_ft, num_epochs=num_epochs)\n",
    "    modelname=glob.glob(model_dir+r'/model_26class_*.pth')\n",
    "    modelname = sorted(modelname)\n",
    "    bestacc=0\n",
    "    best_model=None\n",
    "    for model in modelname:\n",
    "        acc,modeltemp=test_model(model,dname)\n",
    "        if acc>=bestacc:\n",
    "            best_model = modeltemp\n",
    "            bestacc=acc\n",
    "    print('Best_Acc: {:.4f}'.format(bestacc))\n",
    "    print best_model\n",
    "    bestmodel = torch.load(best_model)\n",
    "    #bestmodel = torch.load('./model/model_26class_'+str(num_epochs-1)+'.pth')\n",
    "    count = count +1\n",
    "    model_name='./model/model_best_'+str(count)+'.pth'\n",
    "    torch.save(bestmodel,model_name)\n",
    "#print('All_Best_Acc: {:.4f}'.format(all_bestacc))\n",
    "    #visual_best_model(best_model,dname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
